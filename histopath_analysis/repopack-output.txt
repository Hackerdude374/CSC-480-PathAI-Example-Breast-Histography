================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-12-11T12:07:02.663Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.gitignore
configs/model_config.yaml
configs/training_config.yaml
docker/Dockerfile.api
docker/Dockerfile.frontend
frontend/app.py
frontend/components/visualization.py
mlruns/0/meta.yaml
README.md
requirements.txt
src/api/endpoints.py
src/data/.gitignore
src/data/dataset.py
src/data/preprocessing.py
src/models/combined_model.py
src/models/gnn.py
src/models/mil.py
src/scripts/.repopackignore
src/scripts/evaluate.py
src/scripts/lightning_logs/version_0/hparams.yaml
src/scripts/lightning_logs/version_1/hparams.yaml
src/scripts/lightning_logs/version_2/hparams.yaml
src/scripts/lightning_logs/version_3/hparams.yaml
src/scripts/lightning_logs/version_4/hparams.yaml
src/scripts/organize_dataset.py
src/scripts/train.py
src/utils/metrics.py
src/utils/visualization.py
tests/test_api.py
tests/test_data.py
tests/test_models.py

================================================================
Repository Files
================================================================

================
File: .gitignore
================
data/raw
venv

================
File: configs/model_config.yaml
================
# Model Configuration

# MIL Model Settings
mil_model:
  feature_extractor: "resnet18"
  pretrained: true
  feature_dim: 512
  hidden_dim: 256
  num_classes: 2
  dropout: 0.5
  attention:
    hidden_dim: 128
    num_heads: 1

# GNN Model Settings
gnn_model:
  in_channels: 512
  hidden_channels: 256
  num_layers: 3
  dropout: 0.5
  edge_dim: 32
  aggregation: "mean"

# Combined Model Settings
combined_model:
  fusion_type: "concat"
  fusion_hidden_dim: 256
  use_attention: true
  freeze_feature_extractor: false

# Model Training
training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.0001
  lr_scheduler:
    type: "reduce_on_plateau"
    patience: 5
    factor: 0.5
    min_lr: 0.00001
  early_stopping:
    patience: 10
    min_delta: 0.001

# Model Evaluation
evaluation:
  metrics:
    - accuracy
    - precision
    - recall
    - f1_score
    - roc_auc
  confidence_threshold: 0.5
  num_visualization_samples: 10

================
File: configs/training_config.yaml
================
# Training Configuration

# Data Settings
data:
  train_path: "data/train"
  val_path: "data/val"
  test_path: "data/test"
  patch_size: 50
  num_patches: 100
  overlap: 0.5
  
  # Data Augmentation
  augmentation:
    enabled: true
    horizontal_flip: true
    vertical_flip: true
    rotation: true
    color_jitter:
      brightness: 0.1
      contrast: 0.1
      saturation: 0.1
      hue: 0.1
    random_crop:
      enabled: true
      size: 48
    normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

# Training Process
training:
  seed: 42
  num_workers: 4
  pin_memory: true
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  
  # Optimization
  optimizer:
    type: "adam"
    learning_rate: 0.0001
    weight_decay: 0.0001
    beta1: 0.9
    beta2: 0.999
    
  # Learning Rate Scheduling
  lr_scheduler:
    type: "reduce_on_plateau"
    mode: "min"
    factor: 0.5
    patience: 5
    min_lr: 0.000001
    
  # Early Stopping
  early_stopping:
    monitor: "val_loss"
    patience: 10
    min_delta: 0.001
    mode: "min"

# Logging and Checkpoints
logging:
  project_name: "histopathology-analysis"
  log_every_n_steps: 50
  
  # MLflow settings
  mlflow:
    tracking_uri: "http://localhost:5000"
    experiment_name: "histopathology-training"
    
  # Checkpointing
  checkpointing:
    dirpath: "checkpoints"
    filename: "model-{epoch:02d}-{val_loss:.2f}"
    monitor: "val_loss"
    mode: "min"
    save_top_k: 3
    save_last: true

# Validation
validation:
  val_check_interval: 1.0
  limit_val_batches: 1.0
  
  # Metrics to track
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - roc_auc
    
  # Visualization
  visualize_predictions: true
  num_samples_to_visualize: 10
  save_visualizations: true

================
File: docker/Dockerfile.api
================
# Use Python 3.9 base image
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements file
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/
COPY configs/ ./configs/

# Set environment variables
ENV MODEL_PATH="/app/models"
ENV CONFIG_PATH="/app/configs"
ENV PYTHONPATH="/app"

# Expose port
EXPOSE 8000

# Start FastAPI server
CMD ["uvicorn", "src.api.endpoints:app", "--host", "0.0.0.0", "--port", "8000"]

================
File: docker/Dockerfile.frontend
================
# Use Python 3.9 base image
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements file
COPY frontend/requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy frontend code
COPY frontend/ ./frontend/

# Set environment variables
ENV STREAMLIT_SERVER_PORT=8501
ENV STREAMLIT_SERVER_ADDRESS="0.0.0.0"

# Expose port
EXPOSE 8501

# Start Streamlit server
CMD ["streamlit", "run", "frontend/app.py"]

================
File: frontend/app.py
================
import streamlit as st
import requests
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from PIL import Image
import io
import pandas as pd
from pathlib import Path
import time

# Configure page
st.set_page_config(
    page_title="Histopathology Analysis",
    page_icon="ðŸ”¬",
    layout="wide"
)

# Styling
st.markdown("""
    <style>
    .main { padding: 2rem; }
    .stButton>button { width: 100%; }
    .upload-box {
        border: 2px dashed #4CAF50;
        border-radius: 10px;
        padding: 2rem;
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)

class HistopathologyApp:
    def __init__(self):
        self.API_URL = "http://localhost:8000"
        
    def main(self):
        st.title("ðŸ”¬ Histopathology Image Analysis: by Robert Le, Ashwin Anil, Justin Thomas")
        
        # Sidebar
        self.setup_sidebar()
        
        # Main content
        col1, col2 = st.columns([1, 1.5])
        
        with col1:
            self.image_upload_section()
            
        with col2:
            self.results_section()
    
    def setup_sidebar(self):
        st.sidebar.title("Settings")
        
        # Analysis settings
        st.sidebar.subheader("Analysis Settings")
        self.confidence_threshold = st.sidebar.slider(
            "Confidence Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.05
        )
        
        # Visualization settings
        st.sidebar.subheader("Visualization Settings")
        self.show_attention = st.sidebar.checkbox(
            "Show Attention Heatmap",
            value=True
        )
        
        # Model info
        st.sidebar.subheader("Model Information")
        if st.sidebar.button("Load Model Info"):
            self.display_model_info()
    
    def image_upload_section(self):
        st.subheader("Upload Images")
        
        uploaded_file = st.file_uploader(
            "Choose a histopathology image",
            type=["png", "jpg", "jpeg", "tif", "tiff"]
        )
        
        if uploaded_file is not None:
            try:
                # Display image
                image = Image.open(uploaded_file)
                st.image(image, caption="Uploaded Image", use_column_width=True)
                
                # Process button
                if st.button("Analyze Image"):
                    self.process_image(uploaded_file, image)
                    
            except Exception as e:
                st.error(f"Error opening image: {str(e)}")
    
    def process_image(self, file, image):
        with st.spinner("Analyzing image..."):
            try:
                # Save image to bytes
                img_byte_arr = io.BytesIO()
                image.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                
                # Prepare file for upload
                files = {
                    "file": (
                        file.name,
                        img_byte_arr.getvalue(),
                        "image/png"
                    )
                }
                
                # Make API request
                response = requests.post(
                    f"{self.API_URL}/predict",
                    files=files
                )
                response.raise_for_status()
                
                # Process response
                results = response.json()
                st.session_state.current_results = results
                
                if results.get('status') == 'success':
                    st.success("Analysis complete!")
                else:
                    st.warning(results.get('message', 'Analysis completed with warnings'))
                
            except Exception as e:
                st.error(f"Error during analysis: {str(e)}")
    
    def results_section(self):
        if 'current_results' not in st.session_state:
            st.info("Upload and analyze an image to see results")
            return
        
        results = st.session_state.current_results
        
        # Display predictions
        st.subheader("Analysis Results")
        
        if 'predictions' in results:
            # Confidence score
            confidence = results['predictions']['confidence']
            predicted_class = results['predictions']['predicted_class']
            class_name = "Malignant" if predicted_class == 1 else "Benign"
            
            st.metric(
                "Prediction",
                class_name,
                f"Confidence: {confidence:.1%}"
            )
            
            # Class probabilities
            probs = results['predictions']['class_probabilities']
            fig = go.Figure(data=[
                go.Bar(
                    x=['Benign', 'Malignant'],
                    y=probs,
                    marker_color=['#2ecc71', '#e74c3c']
                )
            ])
            fig.update_layout(
                title="Class Probabilities",
                yaxis_title="Probability",
                showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Attention heatmap
            if self.show_attention and 'visualizations' in results:
                st.subheader("Attention Heatmap")
                if 'attention_heatmap' in results['visualizations']:
                    heatmap = np.array(results['visualizations']['attention_heatmap'])
                    st.image(heatmap, caption="Regions of Interest")
        
        # Display metadata
        if 'metadata' in results:
            st.subheader("Image Information")
            st.json(results['metadata'])
    
    def display_model_info(self):
        try:
            response = requests.get(f"{self.API_URL}/model-info")
            response.raise_for_status()
            
            info = response.json()
            st.sidebar.json(info)
            
        except Exception as e:
            st.sidebar.error(f"Error fetching model info: {str(e)}")

if __name__ == "__main__":
    app = HistopathologyApp()
    app.main()

================
File: frontend/components/visualization.py
================
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import numpy as np
from PIL import Image
import io
from typing import List, Dict, Optional
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

class AttentionMapVisualizer:
    """Component for visualizing attention heatmaps"""
    def __init__(self, colorscale: str = "Viridis"):
        self.colorscale = colorscale

    def plot_attention_heatmap(
        self,
        original_image: Image.Image,
        attention_weights: np.ndarray,
        overlay_alpha: float = 0.5
    ):
        """Display attention heatmap overlaid on original image"""
        # Create columns for side-by-side view
        col1, col2, col3 = st.columns(3)
        
        # Original image
        with col1:
            st.subheader("Original Image")
            st.image(original_image, use_column_width=True)
        
        # Attention heatmap
        with col2:
            st.subheader("Attention Heatmap")
            fig = px.imshow(
                attention_weights,
                color_continuous_scale=self.colorscale
            )
            fig.update_layout(
                coloraxis_showscale=True,
                showlegend=False,
                margin=dict(l=0, r=0, t=0, b=0)
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Overlaid view
        with col3:
            st.subheader("Overlay View")
            st.image(
                self._create_overlay(
                    original_image,
                    attention_weights,
                    overlay_alpha
                ),
                use_column_width=True
            )

    def _create_overlay(
        self,
        image: Image.Image,
        attention: np.ndarray,
        alpha: float
    ) -> Image.Image:
        """Create overlay of attention map on image"""
        # Convert attention to heatmap
        plt.figure(figsize=(10, 10))
        heatmap = plt.imshow(attention, cmap='viridis')
        plt.axis('off')
        
        # Save heatmap to buffer
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
        buf.seek(0)
        plt.close()
        
        # Open heatmap image and resize to match original
        heatmap_img = Image.open(buf).resize(image.size)
        
        # Blend images
        return Image.blend(image, heatmap_img, alpha)

class PredictionVisualizer:
    """Component for visualizing model predictions"""
    def show_prediction_results(
        self,
        probabilities: List[float],
        confidence_threshold: float = 0.5
    ):
        """Display prediction probabilities and confidence"""
        # Prediction bar chart
        fig = go.Figure(data=[
            go.Bar(
                x=['Benign', 'Malignant'],
                y=probabilities,
                marker_color=['#2ecc71', '#e74c3c'],
                text=[f'{p:.1%}' for p in probabilities],
                textposition='auto',
            )
        ])
        
        fig.update_layout(
            title='Prediction Probabilities',
            yaxis_title='Probability',
            yaxis_range=[0, 1],
            showlegend=False
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Confidence indicator
        confidence = max(probabilities)
        st.metric(
            "Prediction Confidence",
            f"{confidence:.1%}",
            delta="Above threshold" if confidence > confidence_threshold else "Below threshold"
        )

class TissueGraphVisualizer:
    """Component for visualizing tissue graph structure"""
    def plot_tissue_graph(
        self,
        nodes: np.ndarray,
        edges: np.ndarray,
        node_features: Optional[np.ndarray] = None
    ):
        """Display tissue graph visualization"""
        st.subheader("Tissue Graph Structure")
        
        # Create graph figure
        fig = go.Figure()
        
        # Add edges
        for (src, dst) in edges:
            fig.add_trace(go.Scatter(
                x=[nodes[src, 0], nodes[dst, 0]],
                y=[nodes[src, 1], nodes[dst, 1]],
                mode='lines',
                line=dict(color='#888', width=1),
                hoverinfo='none'
            ))
        
        # Add nodes
        node_colors = node_features if node_features is not None else '#1f77b4'
        fig.add_trace(go.Scatter(
            x=nodes[:, 0],
            y=nodes[:, 1],
            mode='markers',
            marker=dict(
                size=10,
                color=node_colors,
                colorscale='Viridis',
                showscale=bool(node_features is not None)
            ),
            text=[f'Node {i}' for i in range(len(nodes))],
            hoverinfo='text'
        ))
        
        fig.update_layout(
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
        )
        
        st.plotly_chart(fig, use_container_width=True)

class AnalysisReport:
    """Component for generating analysis reports"""
    def create_report(
        self,
        results: Dict,
        original_image: Image.Image,
        include_graphs: bool = True
    ) -> bytes:
        """Generate PDF report with analysis results"""
        plt.style.use('seaborn')
        fig = plt.figure(figsize=(12, 8))
        
        # Layout
        gs = fig.add_gridspec(2, 2)
        
        # Original image
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.imshow(original_image)
        ax1.set_title('Original Image')
        ax1.axis('off')
        
        # Attention heatmap
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.imshow(results['attention_map'])
        ax2.set_title('Attention Heatmap')
        ax2.axis('off')
        
        # Predictions
        ax3 = fig.add_subplot(gs[1, 0])
        sns.barplot(
            x=['Benign', 'Malignant'],
            y=results['probabilities'],
            ax=ax3
        )
        ax3.set_title('Prediction Probabilities')
        
        # Save to buffer
        buf = io.BytesIO()
        plt.savefig(buf, format='pdf', bbox_inches='tight')
        buf.seek(0)
        
        return buf.getvalue()

def display_batch_results(results: List[Dict]):
    """Display results for batch analysis"""
    # Create summary dataframe
    df = pd.DataFrame([
        {
            'Image': Path(r['image_path']).name,
            'Prediction': 'Malignant' if r['predictions']['predicted_class'] == 1 else 'Benign',
            'Confidence': r['predictions']['confidence'],
            'Processing Time': r['processing_time']
        }
        for r in results
    ])
    
    # Show summary table
    st.dataframe(df)
    
    # Show statistics
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric(
            "Average Confidence",
            f"{df['Confidence'].mean():.1%}"
        )
        
    with col2:
        st.metric(
            "Average Processing Time",
            f"{df['Processing Time'].mean():.2f}s"
        )
    
    # Distribution plot
    fig = px.histogram(
        df,
        x='Confidence',
        color='Prediction',
        nbins=20,
        title='Confidence Distribution'
    )
    st.plotly_chart(fig, use_container_width=True)

================
File: mlruns/0/meta.yaml
================
artifact_location: file:///Z:/PMA/Robert%20Le%20Maddalenas%20unit/Github%20Projects/CSC%20480%20AI%20Project%20PathAI/CSC-480-PathAI-Example-Breast-Histography/histopath_analysis/mlruns/0
creation_time: 1733860894136
experiment_id: '0'
last_update_time: 1733860894136
lifecycle_stage: active
name: Default

================
File: README.md
================
uvicorn src.api.endpoints:app --reload --host 0.0.0.0 --port 8000

streamlit run frontend/app.py

mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 --port 5000

put the kaggle archive.zip under data/raw
unzip it 

then make 3 new folders make processed under data 

then in processed make test, train, and val


then run organize_dataset.py and train.py under scripts folder

================
File: requirements.txt
================
pip install mlflow
pip install fastapi uvicorn
pip install streamlit
pip install torch torchvision pytorch-lightning
pip install plotly opencv-python scikit-learn pandas numpy pillow
pip install hydra-core pytest albumentations
pip install torch-geometric
conda install numpy==1.26.4
conda install pandas
conda install mlflow
pip install scikit-image
pip install torch

pip install python-multipart

================
File: src/api/endpoints.py
================
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import torch
import torchvision.transforms as transforms
import numpy as np
from PIL import Image
import io
import logging
from typing import Dict, List

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Histopathology Analysis API",
    description="API for analyzing histopathology images using MIL-GNN model",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ModelService:
    def __init__(self):
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.transform = self._setup_transform()
        
    def _setup_transform(self):
        return transforms.Compose([
            transforms.Resize((50, 50)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    async def process_image(self, image: Image.Image) -> Dict:
        """Process a single image"""
        try:
            # Since model is not loaded, return dummy predictions
            return {
                'probabilities': [0.6, 0.4],  # Dummy probabilities
                'attention_weights': [[0.5] * 100],  # Dummy attention weights
                'heatmap': np.zeros((224, 224, 3), dtype=np.uint8),  # Dummy heatmap
                'gnn_features': [[0.0] * 256]  # Dummy GNN features
            }
            
        except Exception as e:
            logger.error(f"Error processing image: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Error processing image: {str(e)}"
            )

model_service = ModelService()

@app.post("/predict")
async def predict(file: UploadFile = File(...)) -> Dict:
    try:
        # Log received file info
        print(f"Received file: {file.filename}")
        print(f"Content type: {file.content_type}")
        
        # Validate file type
        if not file.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):
            raise HTTPException(
                status_code=400,
                detail="Invalid file format. Please upload PNG, JPG, or TIFF images."
            )
        
        # Read file contents
        contents = await file.read()
        print(f"Read file size: {len(contents)} bytes")
        
        if len(contents) == 0:
            raise HTTPException(
                status_code=400,
                detail="Empty file received"
            )
        
        # Create BytesIO object and open image
        image_bytes = io.BytesIO(contents)
        try:
            image = Image.open(image_bytes)
            image.load()  # Force load the image data
            print(f"Image opened successfully: size={image.size}, mode={image.mode}")
        except Exception as e:
            print(f"Error opening image: {str(e)}")
            raise HTTPException(
                status_code=400,
                detail=f"Error opening image: {str(e)}"
            )

        # Convert to RGB if needed
        if image.mode != 'RGB':
            image = image.convert('RGB')
            print("Converted image to RGB mode")

        # Process image
        print("Processing image...")
        results = await model_service.process_image(image)
        
        return {
            'status': 'success',
            'predictions': {
                'class_probabilities': results['probabilities'],
                'predicted_class': int(np.argmax(results['probabilities'])),
                'confidence': float(np.max(results['probabilities']))
            },
            'visualizations': {
                'attention_heatmap': results['heatmap'].tolist(),
                'attention_weights': results['attention_weights']
            },
            'metadata': {
                'image_size': image.size,
                'image_mode': image.mode
            }
        }
        
    except Exception as e:
        print(f"Error processing request: {str(e)}")
        logger.error(f"Prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check() -> Dict:
    """Check if the service is healthy"""
    return {
        'status': 'healthy',
        'model_loaded': model_service.model is not None
    }

@app.get("/model-info")
async def get_model_info() -> Dict:
    """Get information about the loaded model"""
    return {
        'model_type': "Dummy Model (No model loaded)",
        'device': str(model_service.device),
        'status': 'Test mode - No model loaded'
    }
    
    


@app.post("/batch-predict")
async def batch_predict(files: List[UploadFile] = File(...)) -> Dict:
    results = []
    for file in files:
        try:
            result = await predict(file)
            results.append({
                'filename': file.filename,
                'predictions': result
            })
        except Exception as e:
            results.append({
                'filename': file.filename,
                'error': str(e)
            })
    return {'results': results}

================
File: src/data/.gitignore
================
raw
processed

================
File: src/data/dataset.py
================
import torch
from torch.utils.data import Dataset
import pytorch_lightning as pl
from torch_geometric.data import Data
from pathlib import Path
import numpy as np
from PIL import Image
import h5py
from typing import Dict, List, Tuple, Optional
import torchvision.transforms as transforms
from .preprocessing import create_tissue_graph, extract_patches

class HistopathologyDataset(Dataset):
    """Dataset class for histopathology images"""
    def __init__(
        self,
        data_dir: str,
        mode: str = 'train',
        patch_size: int = 50,
        num_patches: int = 100,
        transform: Optional[transforms.Compose] = None
    ):
        super().__init__()
        self.data_dir = Path(data_dir)
        self.mode = mode
        self.patch_size = patch_size
        self.num_patches = num_patches
        
        # Default transform if none provided
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((patch_size, patch_size)),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
        else:
            self.transform = transform
        
        # Load image paths and labels
        self.samples = self._load_dataset()

    def _load_dataset(self) -> List[Dict]:
        """Load dataset paths and labels"""
        samples = []
        for class_dir in self.data_dir.glob('**/[0-1]'):
            label = int(class_dir.name)
            for img_path in class_dir.glob('*.png'):
                samples.append({
                    'path': str(img_path),
                    'label': label,
                    'patient_id': img_path.parent.parent.name
                })
        return samples

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict:
        sample = self.samples[idx]
        
        # Load and process image
        image = Image.open(sample['path']).convert('RGB')
        
        # Extract patches
        patches = extract_patches(
            image,
            patch_size=self.patch_size,
            num_patches=self.num_patches
        )
        
        # Apply transforms to patches
        transformed_patches = torch.stack([
            self.transform(patch) for patch in patches
        ])
        
        # Create tissue graph
        graph = create_tissue_graph(transformed_patches)
        
        return {
            'patches': transformed_patches,
            'graph': graph,
            'label': torch.tensor(sample['label'], dtype=torch.long),
            'patient_id': sample['patient_id']
        }

class HistopathologyDataModule(pl.LightningDataModule):
    """PyTorch Lightning data module for histopathology dataset"""
    def __init__(
        self,
        data_dir: str,
        batch_size: int = 32,
        num_workers: int = 4,
        patch_size: int = 50,
        num_patches: int = 100
    ):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.patch_size = patch_size
        self.num_patches = num_patches

    def setup(self, stage: Optional[str] = None):
        """Setup train, validation, and test datasets"""
        # Split data by patient ID for proper validation
        if stage == 'fit' or stage is None:
            self.train_dataset = HistopathologyDataset(
                f"{self.data_dir}/train",
                mode='train',
                patch_size=self.patch_size,
                num_patches=self.num_patches
            )
            self.val_dataset = HistopathologyDataset(
                f"{self.data_dir}/val",
                mode='val',
                patch_size=self.patch_size,
                num_patches=self.num_patches
            )
            
        if stage == 'test' or stage is None:
            self.test_dataset = HistopathologyDataset(
                f"{self.data_dir}/test",
                mode='test',
                patch_size=self.patch_size,
                num_patches=self.num_patches
            )

    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True
        )

    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )

    def test_dataloader(self):
        return torch.utils.data.DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )

================
File: src/data/preprocessing.py
================
import numpy as np
import torch
from PIL import Image
from typing import List, Tuple
from torch_geometric.data import Data
import cv2
from skimage.feature import peak_local_max
from scipy.spatial import Delaunay

# def extract_patches(
#     image: Image.Image,
#     patch_size: int = 50,
#     num_patches: int = 100,
#     overlap: float = 0.5
# ) -> List[Image.Image]:
#     """Extract patches from whole slide image"""
#     width, height = image.size
#     stride = int(patch_size * (1 - overlap))
    
#     # Get all possible patch coordinates
#     x_coords = range(0, width - patch_size + 1, stride)
#     y_coords = range(0, height - patch_size + 1, stride)
    
#     patches = []
#     for x in x_coords:
#         for y in y_coords:
#             patch = image.crop((x, y, x + patch_size, y + patch_size))
#             if _is_tissue_patch(patch):
#                 patches.append(patch)
    
#     # Random sample if we have more patches than needed
#     if len(patches) > num_patches:
#         indices = np.random.choice(
#             len(patches),
#             num_patches,
#             replace=False
#         )
#         patches = [patches[i] for i in indices]
    
#     # Pad with empty patches if we have too few
#     while len(patches) < num_patches:
#         patches.append(Image.fromarray(np.zeros((patch_size, patch_size, 3))))
    
#     return patches
def extract_patches(
    image: Image.Image,
    patch_size: int = 50,
    num_patches: int = 100,
    overlap: float = 0.5
) -> List[Image.Image]:
    """Extract patches from whole slide image"""
    width, height = image.size
    stride = int(patch_size * (1 - overlap))
    
    # Get all possible patch coordinates
    x_coords = range(0, width - patch_size + 1, stride)
    y_coords = range(0, height - patch_size + 1, stride)
    
    patches = []
    for x in x_coords:
        for y in y_coords:
            patch = image.crop((x, y, x + patch_size, y + patch_size))
            if _is_tissue_patch(patch):
                patches.append(patch)
    
    # Random sample if we have more patches than needed
    if len(patches) > num_patches:
        indices = np.random.choice(len(patches), num_patches, replace=False)
        patches = [patches[i] for i in indices]
    
    # Pad with empty patches if we have too few
    while len(patches) < num_patches:
        # Create empty patch with correct data type
        empty_patch = np.zeros((patch_size, patch_size, 3), dtype=np.uint8)  # Change to uint8
        patches.append(Image.fromarray(empty_patch))
    
    return patches


def _is_tissue_patch(patch: Image.Image, threshold: float = 0.1) -> bool:
    """Check if patch contains tissue (not background)"""
    gray = cv2.cvtColor(np.array(patch), cv2.COLOR_RGB2GRAY)
    return np.mean(gray < 220) > threshold


def create_tissue_graph(patches: torch.Tensor) -> Data:
    """Create a graph from tissue patches using nuclei detection"""
    # Convert patches to numpy for OpenCV processing
    patches_np = patches.permute(0, 2, 3, 1).numpy()
    
    # Initialize lists for storing features and positions
    node_features = []
    node_positions = []
    
    for patch in patches_np:
        # Convert to grayscale
        gray = cv2.cvtColor((patch * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
        
        # Detect nuclei using local maxima
        nuclei_coords = peak_local_max(
            -gray,
            min_distance=10,
            threshold_abs=50,
            exclude_border=False
        )
        
        # Extract features for each nucleus
        for coord in nuclei_coords:
            y, x = coord
            # Get patch features around nucleus
            feature_patch = patch[
                max(0, y-5):min(patch.shape[0], y+5),
                max(0, x-5):min(patch.shape[1], x+5)
            ]
            # Use mean color and intensity as features
            features = np.concatenate([
                np.mean(feature_patch, axis=(0, 1)),
                np.array([gray[y, x] / 255.0])
            ])
            node_features.append(features)
            node_positions.append([x, y])

    # If no nodes were detected, create a dummy node
    if len(node_features) == 0:
        node_features = [[0.0] * 4]  # 3 color channels + 1 intensity
        node_positions = [[0.0, 0.0]]

    # Convert lists to numpy arrays first, then to tensors
    node_features = np.array(node_features, dtype=np.float32)
    node_positions = np.array(node_positions, dtype=np.float32)
    
    # Convert to tensors
    node_features = torch.from_numpy(node_features)
    node_positions = torch.from_numpy(node_positions)
    
    # Create edges using Delaunay triangulation
    if len(node_positions) > 3:
        tri = Delaunay(node_positions.numpy())
        edges = []
        for simplex in tri.simplices:
            for i in range(3):
                edges.append([simplex[i], simplex[(i + 1) % 3]])
                edges.append([simplex[(i + 1) % 3], simplex[i]])
        edge_index = torch.tensor(edges, dtype=torch.long).t()
    else:
        # Fallback for too few nodes: create fully connected graph
        num_nodes = len(node_positions)
        edges = [
            [i, j] for i in range(num_nodes) for j in range(num_nodes)
            if i != j
        ]
        edge_index = torch.tensor(edges, dtype=torch.long).t()
    
    # Create PyG Data object
    graph = Data(
        x=node_features,
        edge_index=edge_index,
        pos=node_positions
    )
    
    return graph

def normalize_staining(
    image: np.ndarray,
    target_means: Tuple[float, float, float] = (0.8, 0.8, 0.8),
    target_stds: Tuple[float, float, float] = (0.1, 0.1, 0.1)
) -> np.ndarray:
    """Normalize H&E staining colors"""
    # Convert to optical density
    od = -np.log((image.astype(float) + 1) / 256)
    
    # Calculate current means and stds
    means = np.mean(od, axis=(0, 1))
    stds = np.std(od, axis=(0, 1))
    
    # Normalize
    normalized = np.zeros_like(od)
    for i in range(3):
        normalized[:, :, i] = ((od[:, :, i] - means[i]) 
                             * (target_stds[i] / stds[i]) 
                             + target_means[i])
    
    # Convert back to RGB
    normalized = np.exp(-normalized) * 256 - 1
    normalized = np.clip(normalized, 0, 255).astype(np.uint8)
    
    return normalized

def augment_patch(
    patch: Image.Image,
    augmentation_params: dict
) -> Image.Image:
    """Apply augmentations to a patch"""
    # Convert to numpy array
    patch_np = np.array(patch)
    
    if augmentation_params.get('flip_horizontal', False):
        if np.random.rand() > 0.5:
            patch_np = np.fliplr(patch_np)
            
    if augmentation_params.get('flip_vertical', False):
        if np.random.rand() > 0.5:
            patch_np = np.flipud(patch_np)
            
    if augmentation_params.get('rotate', False):
        angle = np.random.randint(0, 360)
        patch_np = rotate_image(patch_np, angle)
        
    if augmentation_params.get('color_jitter', False):
        patch_np = apply_color_jitter(patch_np)
        
    return Image.fromarray(patch_np)

def rotate_image(image: np.ndarray, angle: float) -> np.ndarray:
    """Rotate image by given angle"""
    height, width = image.shape[:2]
    center = (width // 2, height // 2)
    
    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(
        image,
        rotation_matrix,
        (width, height),
        borderMode=cv2.BORDER_REFLECT
    )
    
    return rotated

def apply_color_jitter(
    image: np.ndarray,
    brightness: float = 0.1,
    contrast: float = 0.1,
    saturation: float = 0.1,
    hue: float = 0.1
) -> np.ndarray:
    """Apply color jittering to image"""
    # Convert to HSV for easier manipulation
    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(np.float32)
    
    # Random adjustments
    hsv[:, :, 0] += np.random.uniform(-hue, hue) * 180  # Hue
    hsv[:, :, 1] *= np.random.uniform(1-saturation, 1+saturation)  # Saturation
    hsv[:, :, 2] *= np.random.uniform(1-brightness, 1+brightness)  # Value
    
    # Clip values
    hsv[:, :, 0] = np.clip(hsv[:, :, 0], 0, 180)
    hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)
    hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255)
    
    # Convert back to RGB
    image = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
    
    # Adjust contrast
    mean = np.mean(image, axis=(0, 1), keepdims=True)
    image = (image - mean) * np.random.uniform(1-contrast, 1+contrast) + mean
    image = np.clip(image, 0, 255).astype(np.uint8)
    
    return image

================
File: src/models/combined_model.py
================
import torch
import torch.nn as nn
import pytorch_lightning as pl
from typing import Dict, Tuple

from .mil import MILModel
from .gnn import GNNModel

class CombinedModel(pl.LightningModule):
    """Combined MIL-GNN model for comprehensive tissue analysis"""
    def __init__(
        self,
        num_classes: int = 2,
        mil_feature_dim: int = 512,
        gnn_hidden_dim: int = 256,
        learning_rate: float = 1e-4
    ):
        super().__init__()
        self.save_hyperparameters()
        
        # Initialize sub-models
        self.mil_model = MILModel(num_classes=num_classes)
        self.gnn_model = GNNModel(
            in_channels=4,  # Change from 512 to 4
            hidden_channels=256,
            num_classes=num_classes
        )
        
        # Fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(num_classes * 2, num_classes),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(num_classes, num_classes)
        )

    def forward(self, batch: Dict) -> Tuple[torch.Tensor, Dict]:
        # MIL forward pass
        mil_logits, mil_attention = self.mil_model(batch['patches'])
        
        # GNN forward pass
        gnn_logits, gnn_features = self.gnn_model(batch['graph'])
        
        # Combine predictions
        combined_features = torch.cat([mil_logits, gnn_logits], dim=1)
        final_logits = self.fusion(combined_features)
        
        # Store intermediate results
        outputs = {
            'mil_attention': mil_attention,
            'gnn_features': gnn_features,
            'mil_logits': mil_logits,
            'gnn_logits': gnn_logits
        }
        
        return final_logits, outputs

    def training_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:
        logits, _ = self(batch)
        loss = nn.CrossEntropyLoss()(logits, batch['label'])
        
        # Log metrics
        self.log('train_loss', loss)
        acc = (logits.argmax(1) == batch['label']).float().mean()
        self.log('train_acc', acc)
        
        return loss

    def validation_step(self, batch: Dict, batch_idx: int) -> Dict:
        logits, outputs = self(batch)
        loss = nn.CrossEntropyLoss()(logits, batch['label'])
        
        # Log metrics
        self.log('val_loss', loss)
        acc = (logits.argmax(1) == batch['label']).float().mean()
        self.log('val_acc', acc)
        
        # Log sub-model metrics
        mil_acc = (outputs['mil_logits'].argmax(1) == batch['label']).float().mean()
        gnn_acc = (outputs['gnn_logits'].argmax(1) == batch['label']).float().mean()
        self.log('val_mil_acc', mil_acc)
        self.log('val_gnn_acc', gnn_acc)
        
        return {
            'val_loss': loss,
            'val_acc': acc,
            'val_mil_acc': mil_acc,
            'val_gnn_acc': gnn_acc
        }

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.hparams.learning_rate
        )
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler,
            'monitor': 'val_loss'
        }

================
File: src/models/gnn.py
================
import torch
import torch.nn as nn
import pytorch_lightning as pl
from torch_geometric.nn import GCNConv, global_mean_pool
from typing import Dict, Tuple

class GNNModel(pl.LightningModule):
    """Graph Neural Network for tissue structure analysis"""
    def __init__(
        self,
        in_channels: int = 512,
        hidden_channels: int = 256,
        num_classes: int = 2,
        num_layers: int = 3,
        learning_rate: float = 1e-4
    ):
        super().__init__()
        self.save_hyperparameters()
        
        # GNN layers
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(in_channels, hidden_channels))
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))
        self.convs.append(GCNConv(hidden_channels, hidden_channels))
        
        # MLP for final classification
        self.mlp = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_channels, num_classes)
        )

    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor]:
        x, edge_index, batch = data.x, data.edge_index, data.batch
        
        # Apply GNN layers
        for conv in self.convs[:-1]:
            x = conv(x, edge_index)
            x = torch.relu(x)
            x = torch.nn.functional.dropout(x, p=0.5, training=self.training)
            
        x = self.convs[-1](x, edge_index)
        
        # Global pooling
        x = global_mean_pool(x, batch)
        
        # Classification
        logits = self.mlp(x)
        
        return logits, x

    def training_step(self, batch, batch_idx) -> torch.Tensor:
        logits, _ = self(batch)
        loss = nn.CrossEntropyLoss()(logits, batch.y)
        
        # Log metrics
        self.log('train_loss', loss)
        acc = (logits.argmax(1) == batch.y).float().mean()
        self.log('train_acc', acc)
        
        return loss

    def validation_step(self, batch, batch_idx) -> Dict:
        logits, _ = self(batch)
        loss = nn.CrossEntropyLoss()(logits, batch.y)
        
        # Log metrics
        self.log('val_loss', loss)
        acc = (logits.argmax(1) == batch.y).float().mean()
        self.log('val_acc', acc)
        
        return {'val_loss': loss, 'val_acc': acc}

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.hparams.learning_rate
        )
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler,
            'monitor': 'val_loss'
        }

================
File: src/models/mil.py
================
import torch
import torch.nn as nn
import pytorch_lightning as pl
from torchvision import models
from typing import Tuple, Dict

class AttentionPool(nn.Module):
    """Attention pooling layer for MIL"""
    def __init__(self, feature_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        attention_weights = self.attention(x)
        attention_weights = torch.softmax(attention_weights, dim=1)
        weighted_features = torch.sum(x * attention_weights, dim=1)
        return weighted_features, attention_weights

class MILModel(pl.LightningModule):
    """Multiple Instance Learning model for histopathology analysis"""
    def __init__(self, num_classes: int = 2, learning_rate: float = 1e-4):
        super().__init__()
        self.save_hyperparameters()
        
        # Feature extractor (ResNet18 backbone)
        resnet = models.resnet18(pretrained=True)
        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])
        self.feature_dim = 512
        
        # Attention pooling
        self.attention_pool = AttentionPool(self.feature_dim)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size, num_instances = x.size(0), x.size(1)
        x = x.view(-1, *x.size()[2:])
        
        # Extract features
        features = self.feature_extractor(x)
        features = features.view(batch_size, num_instances, -1)
        
        # Pool features with attention
        pooled_features, attention_weights = self.attention_pool(features)
        
        # Classify
        logits = self.classifier(pooled_features)
        return logits, attention_weights

    def training_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:
        images, labels = batch['image'], batch['label']
        logits, _ = self(images)
        loss = nn.CrossEntropyLoss()(logits, labels)
        
        # Log metrics
        self.log('train_loss', loss)
        acc = (logits.argmax(1) == labels).float().mean()
        self.log('train_acc', acc)
        
        return loss

    def validation_step(self, batch: Dict, batch_idx: int) -> Dict:
        images, labels = batch['image'], batch['label']
        logits, attention = self(images)
        loss = nn.CrossEntropyLoss()(logits, labels)
        
        # Log metrics
        self.log('val_loss', loss)
        acc = (logits.argmax(1) == labels).float().mean()
        self.log('val_acc', acc)
        
        return {'val_loss': loss, 'val_acc': acc}

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.hparams.learning_rate
        )
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler,
            'monitor': 'val_loss'
        }

================
File: src/scripts/.repopackignore
================
lightning

================
File: src/scripts/evaluate.py
================
import pytorch_lightning as pl
import hydra
from omegaconf import DictConfig
import pandas as pd
from pathlib import Path
import json
import torch
from tqdm import tqdm
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import logging

from src.models.combined_model import CombinedModel
from src.data.dataset import HistopathologyDataModule
from src.utils.metrics import MetricsTracker, calculate_confidence_metrics
from src.utils.visualization import (
    create_attention_heatmap,
    visualize_tissue_graph,
    plot_prediction_confidence
)

# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

def evaluate(
    model_checkpoint_path: str,
    test_data_dir: str,
    output_dir: Path
):
    """
    Evaluate trained model on test dataset
    """
    # Load model from checkpoint
    logger.info(f"Loading model from: {model_checkpoint_path}")
    try:
        model = CombinedModel.load_from_checkpoint(model_checkpoint_path)
        model.eval()
    except Exception as e:
        logger.error(f"Error loading model: {str(e)}")
        raise RuntimeError("Failed to load model")

    # Initialize data module
    datamodule = HistopathologyDataModule(
        data_dir=test_data_dir,
        batch_size=32,
        num_workers=4,
        patch_size=50,
        num_patches=100
    )
    datamodule.setup('test')

    # Initialize metrics tracker
    metrics_tracker = MetricsTracker()
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Evaluate model
    results = []
    with torch.no_grad():
        for batch in tqdm(datamodule.test_dataloader(), desc="Evaluating"):
            # Move batch to device
            batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # Get predictions
            logger.debug("Forwarding batch through model")
            logits, outputs = model(batch)
            probabilities = torch.softmax(logits, dim=1)
            predictions = torch.argmax(logits, dim=1)
            
            # Update metrics
            logger.debug("Updating metrics")
            metrics_tracker.update(
                predictions.cpu(),
                batch['label'].cpu(),
                probabilities.cpu(),
                outputs['mil_attention'].cpu()
            )
            
            # Store results for each sample
            for i in range(len(predictions)):
                result = {
                    'patient_id': batch['patient_id'][i],
                    'true_label': batch['label'][i].item(),
                    'predicted_label': predictions[i].item(),
                    'confidence': probabilities[i].max().item(),
                    'probabilities': probabilities[i].tolist(),
                    'attention_weights': outputs['mil_attention'][i].tolist()
                }
                results.append(result)
                
                # Save visualizations if configured
                if cfg.evaluation.save_visualizations:
                    logger.debug("Saving visualizations")
                    patient_dir = output_dir / 'visualizations' / batch['patient_id'][i]
                    patient_dir.mkdir(parents=True, exist_ok=True)
                    
                    # Attention heatmap
                    attention_map = create_attention_heatmap(
                        batch['patches'][i],
                        outputs['mil_attention'][i]
                    )
                    plt.imsave(
                        patient_dir / 'attention_heatmap.png',
                        attention_map
                    )
                    
                    # Tissue graph
                    graph_fig = visualize_tissue_graph(
                        batch['graph'],
                        outputs['gnn_features'][i]
                    )
                    graph_fig.write_html(patient_dir / 'tissue_graph.html')
    
    # Compute and save metrics
    logger.info("Computing and saving metrics")
    metrics = metrics_tracker.compute_metrics()
    metrics.update(calculate_confidence_metrics(
        np.array([r['probabilities'] for r in results])
    ))
    
    # Save metrics
    with open(output_dir / 'metrics.json', 'w') as f:
        json.dump(metrics, f, indent=4)
    
    # Create and save confusion matrix
    logger.info("Generating and saving confusion matrix")
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(
        [r['true_label'] for r in results],
        [r['predicted_label'] for r in results]
    )
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['Benign', 'Malignant'],
        yticklabels=['Benign', 'Malignant']
    )
    plt.title('Confusion Matrix')
    plt.savefig(output_dir / 'confusion_matrix.png')
    plt.close()
    
    # Generate classification report
    logger.info("Generating and saving classification report")
    report = classification_report(
        [r['true_label'] for r in results],
        [r['predicted_label'] for r in results],
        target_names=['Benign', 'Malignant'],
        output_dict=True
    )
    pd.DataFrame(report).to_csv(output_dir / 'classification_report.csv')
    
    # Create confidence distribution plot
    logger.info("Generating and saving confidence distribution plot")
    plt.figure(figsize=(10, 6))
    sns.histplot(
        data=pd.DataFrame(results),
        x='confidence',
        hue='true_label',
        bins=30,
        multiple="layer"
    )
    plt.title('Confidence Distribution by Class')
    plt.savefig(output_dir / 'confidence_distribution.png')
    plt.close()
    
    # Save detailed results
    logger.info("Saving detailed results")
    pd.DataFrame(results).to_csv(output_dir / 'detailed_results.csv', index=False)
    
    # Log results to MLflow
    logger.info("Logging results to MLflow")
    with mlflow.start_run(run_id=run_id):
        mlflow.log_metrics(metrics)
        mlflow.log_artifacts(str(output_dir))
        
        # Log confusion matrix as a figure
        mlflow.log_figure(
            plt.figure(figsize=(10, 8)),
            'confusion_matrix.png'
        )
    
    # Print summary
    print("\nEvaluation Results:")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"ROC AUC: {metrics['roc_auc']:.4f}")
    print(f"Mean Confidence: {metrics['mean_confidence']:.4f}")
    print(f"\nDetailed results saved to: {output_dir}")

def analyze_failures(results_df: pd.DataFrame, output_dir: Path):
    """Analyze misclassified cases"""
    misclassified = results_df[
        results_df['true_label'] != results_df['predicted_label']
    ]
    
    # Analyze confidence distribution for misclassified cases
    plt.figure(figsize=(10, 6))
    sns.histplot(data=misclassified, x='confidence', bins=20)
    plt.title('Confidence Distribution for Misclassified Cases')
    plt.savefig(output_dir / 'misclassified_confidence.png')
    plt.close()
    
    # Analyze patterns in misclassification
    error_analysis = {
        'total_misclassified': len(misclassified),
        'false_positives': len(misclassified[
            misclassified['predicted_label'] == 1
        ]),
        'false_negatives': len(misclassified[
            misclassified['predicted_label'] == 0
        ]),
        'mean_confidence': misclassified['confidence'].mean(),
        'high_confidence_errors': len(misclassified[
            misclassified['confidence'] > 0.9
        ])
    }
    
    # Save error analysis
    with open(output_dir / 'error_analysis.json', 'w') as f:
        json.dump(error_analysis, f, indent=4)
        
        
if __name__ == "__main__":
    evaluate(
        model_checkpoint_path="histopath_analysis/models/latest.ckpt",
        test_data_dir="histopath_analysis/src/data/processed/test",
        output_dir=Path("histopath_analysis/evaluation")
    )

================
File: src/scripts/lightning_logs/version_0/hparams.yaml
================
num_classes: 2
mil_feature_dim: 512
gnn_hidden_dim: 256
learning_rate: 0.0001

================
File: src/scripts/lightning_logs/version_1/hparams.yaml
================
num_classes: 2
mil_feature_dim: 512
gnn_hidden_dim: 256
learning_rate: 0.0001

================
File: src/scripts/lightning_logs/version_2/hparams.yaml
================
num_classes: 2
mil_feature_dim: 512
gnn_hidden_dim: 256
learning_rate: 0.0001

================
File: src/scripts/lightning_logs/version_3/hparams.yaml
================
num_classes: 2
mil_feature_dim: 512
gnn_hidden_dim: 256
learning_rate: 0.0001

================
File: src/scripts/lightning_logs/version_4/hparams.yaml
================
num_classes: 2
mil_feature_dim: 512
gnn_hidden_dim: 256
learning_rate: 0.0001

================
File: src/scripts/organize_dataset.py
================
from pathlib import Path
import shutil
import random
import os
#THIS ORGANIZES THE DATASET IN THE TRAIN FOLDER
def organize_dataset(
    source_dir = r"C:\GITHUB PROJECTS DO HERE C\CSC 480 AI PAthAI CODE PROJECT\CSC-480-PathAI-Example-Breast-Histography\histopath_analysis\src\data\raw",
    dest_dir = r"C:\GITHUB PROJECTS DO HERE C\CSC 480 AI PAthAI CODE PROJECT\CSC-480-PathAI-Example-Breast-Histography\histopath_analysis\src\data\processed",
    split_ratio={"train": 0.7, "val": 0.15, "test": 0.15}
):
    print(f"Current working directory: {os.getcwd()}")
    print(f"Source directory: {source_dir}")
    print(f"Destination directory: {dest_dir}")
    print(f"Split ratios: {split_ratio}")

    source_path = Path(source_dir)
    if not source_path.exists():
        print(f"ERROR: Source directory '{source_path}' does not exist.")
        return

    print("\nScanning for patient directories...")
    patient_dirs = list(source_path.glob("[0-9]*"))
    print(f"Found {len(patient_dirs)} patient directories:")
    for patient_dir in patient_dirs:
        print(f"  - {patient_dir}")

    if len(patient_dirs) == 0:
        print("No patient directories found. Please check your source directory structure.")
        return

    # Randomly shuffle and split patients
    print("\nShuffling and splitting patient directories...")
    random.seed(42)  # For reproducibility
    random.shuffle(patient_dirs)
    n_patients = len(patient_dirs)
    n_train = int(n_patients * split_ratio["train"])
    n_val = int(n_patients * split_ratio["val"])

    splits = {
        "train": patient_dirs[:n_train],
        "val": patient_dirs[n_train:n_train + n_val],
        "test": patient_dirs[n_train + n_val:]
    }

    for split_name, split_dirs in splits.items():
        print(f"Split '{split_name}' has {len(split_dirs)} patient directories.")

    # Copy files maintaining structure
    print("\nCopying files to destination...")
    for split_name, patient_list in splits.items():
        print(f"\nProcessing split: {split_name}")
        for patient_dir in patient_list:
            print(f"  Processing patient directory: {patient_dir}")
            for class_dir in patient_dir.glob("[0-1]"):
                if not class_dir.is_dir():
                    print(f"    Skipping non-directory: {class_dir}")
                    continue

                print(f"    Found class directory: {class_dir}")
                dest_class_dir = Path(dest_dir) / split_name / class_dir.name
                dest_class_dir.mkdir(parents=True, exist_ok=True)

                for img_path in class_dir.glob("*.png"):
                    if not img_path.is_file():
                        print(f"      Skipping non-file: {img_path}")
                        continue

                    print(f"      Copying image: {img_path}")
                    dest_path = dest_class_dir / f"{patient_dir.name}_{img_path.name}"
                    shutil.copy2(img_path, dest_path)

    print("\nDataset organization complete.")
    for split_name, patient_list in splits.items():
        print(f"\n{split_name.capitalize()} split:")
        print(f"  Number of patients: {len(patient_list)}")

if __name__ == "__main__":
    organize_dataset()
    
# Dataset organization complete.

# Train split:
#   Number of patients: 195

# Val split:
#   Number of patients: 41

# Test split:
#   Number of patients: 43

================
File: src/scripts/train.py
================
import sys
from pathlib import Path
# Add project root to Python path
sys.path.append(str(Path(__file__).parents[2]))

import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader
import mlflow.pytorch
import logging
from torch_geometric.data import Batch

from src.data.dataset import HistopathologyDataset
from src.models.combined_model import CombinedModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def collate_fn(batch):
    """Custom collate function for handling both image patches and graphs"""
    patches = torch.stack([item['patches'] for item in batch])
    labels = torch.stack([item['label'] for item in batch])
    graphs = Batch.from_data_list([item['graph'] for item in batch])
    
    return {
        'patches': patches,
        'graph': graphs,
        'label': labels
    }

def train():
    # Get absolute paths
    project_root = Path(__file__).parents[2]
    train_path = r"C:\GITHUB PROJECTS DO HERE C\CSC 480 AI PAthAI CODE PROJECT\CSC-480-PathAI-Example-Breast-Histography\histopath_analysis\src\data\processed\train"
    val_path = r"C:\GITHUB PROJECTS DO HERE C\CSC 480 AI PAthAI CODE PROJECT\CSC-480-PathAI-Example-Breast-Histography\histopath_analysis\src\data\processed\val"
    print(f"Train Path: {train_path}")
    print(f"Validation Path: {val_path}")
    train_dataset = HistopathologyDataset(data_dir=str(train_path), mode='train')
    val_dataset = HistopathologyDataset(data_dir=str(val_path), mode='val')

    logger.info(f"Train dataset size: {len(train_dataset)}")
    logger.info(f"Validation dataset size: {len(val_dataset)}")
    

    # Initialize datasets
    train_dataset = HistopathologyDataset(
        data_dir=str(train_path),
        mode='train'
    )
    val_dataset = HistopathologyDataset(
        data_dir=str(val_path),
        mode='val'
    )

    logger.info(f"Train dataset size: {len(train_dataset)}")
    logger.info(f"Val dataset size: {len(val_dataset)}")

    # Create data loaders with custom collate function
    train_loader = DataLoader(
        train_dataset,
        batch_size=32,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_fn,
        persistent_workers=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=32,
        shuffle=False,
        num_workers=4,
        collate_fn=collate_fn,
        persistent_workers=True
    )

    # Initialize model
    model = CombinedModel()
    logger.info("Model initialized")

    # Create checkpoint directory
    checkpoint_dir = project_root / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)

    # Setup training
    trainer = pl.Trainer(
        max_epochs=10,
        accelerator='auto',
        devices=1,
        log_every_n_steps=10,
        callbacks=[
            pl.callbacks.ModelCheckpoint(
                dirpath=str(checkpoint_dir),
                filename="model-{epoch:02d}-{val_loss:.2f}",
                save_top_k=3,
                monitor="val_loss"
            ),
            pl.callbacks.EarlyStopping(
                monitor="val_loss",
                patience=3,
                mode="min"
            )
        ]
    )

    # Train
    logger.info("Starting model training...")
    trainer.fit(model, train_loader, val_loader)

    # Create models directory
    models_dir = project_root / "models"
    models_dir.mkdir(exist_ok=True)

    # Save model with MLflow
    logger.info("Saving model...")
    mlflow.pytorch.save_model(model, str(models_dir / "latest"))
    logger.info("Training complete!")

    # Save training metadata
    metadata = {
        'train_size': len(train_dataset),
        'val_size': len(val_dataset),
        'epochs': trainer.current_epoch,
        'train_loss': trainer.callback_metrics.get('train_loss', float('nan')),
        'val_loss': trainer.callback_metrics.get('val_loss', float('nan')),
        'val_acc': trainer.callback_metrics.get('val_acc', float('nan'))
    }
    
    # Save metadata to file
    metadata_file = models_dir / "training_metadata.json"
    import json
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=4)
    
    logger.info(f"Training metadata saved to {metadata_file}")

if __name__ == "__main__":
    train()

================
File: src/utils/metrics.py
================
import torch
import numpy as np
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix
)
from typing import Dict, Tuple, List
import logging

logger = logging.getLogger(__name__)

class MetricsTracker:
    """Track and compute various evaluation metrics"""
    def __init__(self):
        self.reset()
        
    def reset(self):
        """Reset all metrics"""
        self.predictions = []
        self.true_labels = []
        self.probabilities = []
        self.attention_weights = []
        
    def update(
        self,
        preds: torch.Tensor,
        labels: torch.Tensor,
        probs: torch.Tensor,
        attention: torch.Tensor = None
    ):
        """Update metrics with batch results"""
        self.predictions.extend(preds.cpu().numpy())
        self.true_labels.extend(labels.cpu().numpy())
        self.probabilities.extend(probs.cpu().numpy())
        if attention is not None:
            self.attention_weights.extend(attention.cpu().numpy())
            
    def compute_metrics(self) -> Dict:
        """Compute all metrics"""
        try:
            metrics = {
                'accuracy': accuracy_score(
                    self.true_labels,
                    self.predictions
                ),
                'precision': precision_score(
                    self.true_labels,
                    self.predictions,
                    average='weighted'
                ),
                'recall': recall_score(
                    self.true_labels,
                    self.predictions,
                    average='weighted'
                ),
                'f1': f1_score(
                    self.true_labels,
                    self.predictions,
                    average='weighted'
                ),
                'roc_auc': roc_auc_score(
                    self.true_labels,
                    self.probabilities[:, 1]  # Probability of positive class
                ),
                'confusion_matrix': confusion_matrix(
                    self.true_labels,
                    self.predictions
                ).tolist()
            }
            
            # Add attention statistics if available
            if self.attention_weights:
                metrics.update(self._compute_attention_metrics())
                
            return metrics
            
        except Exception as e:
            logger.error(f"Error computing metrics: {str(e)}")
            return {}
            
    def _compute_attention_metrics(self) -> Dict:
        """Compute attention-specific metrics"""
        attention_array = np.array(self.attention_weights)
        return {
            'attention_mean': float(np.mean(attention_array)),
            'attention_std': float(np.std(attention_array)),
            'attention_entropy': float(self._compute_attention_entropy(attention_array))
        }
        
    @staticmethod
    def _compute_attention_entropy(attention: np.ndarray) -> float:
        """Compute entropy of attention weights"""
        epsilon = 1e-10
        attention = np.clip(attention, epsilon, 1.0)
        return float(-np.sum(attention * np.log(attention)) / len(attention))

def calculate_confidence_metrics(
    probabilities: np.ndarray,
    threshold: float = 0.5
) -> Dict:
    """Calculate confidence-based metrics"""
    confidence_scores = np.max(probabilities, axis=1)
    high_confidence = confidence_scores >= threshold
    
    return {
        'mean_confidence': float(np.mean(confidence_scores)),
        'high_confidence_ratio': float(np.mean(high_confidence)),
        'confidence_std': float(np.std(confidence_scores))
    }

def compute_class_metrics(
    true_labels: np.ndarray,
    predictions: np.ndarray,
    probabilities: np.ndarray,
    class_names: List[str]
) -> Dict:
    """Compute per-class metrics"""
    class_metrics = {}
    
    for i, class_name in enumerate(class_names):
        class_mask = true_labels == i
        class_metrics[class_name] = {
            'precision': precision_score(
                true_labels == i,
                predictions == i,
                average='binary'
            ),
            'recall': recall_score(
                true_labels == i,
                predictions == i,
                average='binary'
            ),
            'f1': f1_score(
                true_labels == i,
                predictions == i,
                average='binary'
            ),
            'support': int(np.sum(class_mask))
        }
        
    return class_metrics

def calibration_metrics(
    probabilities: np.ndarray,
    true_labels: np.ndarray,
    num_bins: int = 10
) -> Tuple[float, np.ndarray, np.ndarray]:
    """
    Compute calibration metrics including ECE (Expected Calibration Error)
    """
    confidences = np.max(probabilities, axis=1)
    predictions = np.argmax(probabilities, axis=1)
    accuracies = predictions == true_labels
    
    # Create confidence bins
    bin_boundaries = np.linspace(0, 1, num_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    ece = 0.0
    bin_confidences = []
    bin_accuracies = []
    
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        # Find predictions in current bin
        in_bin = np.logical_and(
            confidences > bin_lower,
            confidences <= bin_upper
        )
        
        if any(in_bin):
            bin_conf = np.mean(confidences[in_bin])
            bin_acc = np.mean(accuracies[in_bin])
            bin_size = np.sum(in_bin)
            
            ece += bin_size * np.abs(bin_conf - bin_acc)
            bin_confidences.append(bin_conf)
            bin_accuracies.append(bin_acc)
            
    ece = ece / len(confidences)
    
    return ece, np.array(bin_confidences), np.array(bin_accuracies)

================
File: src/utils/visualization.py
================
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import torch
import cv2
from typing import List, Tuple, Optional
import plotly.graph_objects as go
import io

def create_attention_heatmap(
    original_image: Image.Image,
    attention_weights: torch.Tensor,
    patch_size: int = 50,
    alpha: float = 0.6
) -> np.ndarray:
    """Create attention heatmap overlay on original image"""
    # Convert image to numpy array
    image_np = np.array(original_image)
    
    # Reshape attention weights to match image grid
    grid_size = int(np.sqrt(len(attention_weights)))
    attention_map = attention_weights.reshape(grid_size, grid_size)
    
    # Resize attention map to match image size
    attention_map = cv2.resize(
        attention_map,
        (image_np.shape[1], image_np.shape[0]),
        interpolation=cv2.INTER_LINEAR
    )
    
    # Normalize attention map
    attention_map = (attention_map - attention_map.min()) / (
        attention_map.max() - attention_map.min()
    )
    
    # Create heatmap
    heatmap = cv2.applyColorMap(
        (attention_map * 255).astype(np.uint8),
        cv2.COLORMAP_JET
    )
    
    # Overlay heatmap on original image
    overlay = cv2.addWeighted(
        image_np,
        1 - alpha,
        heatmap,
        alpha,
        0
    )
    
    return overlay

def plot_prediction_confidence(
    probabilities: np.ndarray,
    class_names: List[str]
) -> go.Figure:
    """Create bar plot of prediction confidence"""
    fig = go.Figure(data=[
        go.Bar(
            x=class_names,
            y=probabilities,
            marker_color=['#2ecc71', '#e74c3c'],
            text=[f'{p:.2%}' for p in probabilities],
            textposition='auto'
        )
    ])
    
    fig.update_layout(
        title='Prediction Confidence',
        xaxis_title='Class',
        yaxis_title='Probability',
        yaxis_range=[0, 1],
        showlegend=False
    )
    
    return fig

def visualize_tissue_graph(
    graph: torch.Tensor,
    node_colors: Optional[torch.Tensor] = None,
    edge_weights: Optional[torch.Tensor] = None
) -> go.Figure:
    """Create interactive visualization of tissue graph"""
    # Get node positions
    pos = graph.pos.numpy()
    
    # Create node trace
    node_trace = go.Scatter(
        x=pos[:, 0],
        y=pos[:, 1],
        mode='markers',
        marker=dict(
            size=10,
            color=node_colors.numpy() if node_colors is not None else '#1f77b4',
            colorscale='Viridis',
            showscale=True if node_colors is not None else False
        ),
        text=[f'Node {i}' for i in range(len(pos))],
        hoverinfo='text'
    )
    
    # Create edge traces
    edge_traces = []
    edges = graph.edge_index.t().numpy()
    
    if edge_weights is not None:
        weights = edge_weights.numpy()
    else:
        weights = np.ones(len(edges))
    
    for (src, dst), weight in zip(edges, weights):
        edge_traces.append(go.Scatter(
            x=[pos[src, 0], pos[dst, 0]],
            y=[pos[src, 1], pos[dst, 1]],
            mode='lines',
            line=dict(
                width=weight * 2,
                color='#888'
            ),
            hoverinfo='none'
        ))
    
    # Combine traces
    fig = go.Figure(data=[*edge_traces, node_trace])
    
    fig.update_layout(
        title='Tissue Graph Structure',
        showlegend=False,
        hovermode='closest',
        margin=dict(b=20, l=5, r=5, t=40),
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
    )
    
    return fig

def create_analysis_report(
    image_path: str,
    predictions: dict,
    attention_map: np.ndarray,
    graph_fig: go.Figure
) -> bytes:
    """Create PDF report with analysis results"""
    plt.style.use('seaborn')
    
    # Create figure with subplots
    fig = plt.figure(figsize=(12, 8))
    gs = fig.add_gridspec(2, 2)
    
    # Original image with attention overlay
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.imshow(attention_map)
    ax1.set_title('Attention Heatmap')
    ax1.axis('off')
    
    # Prediction confidence
    ax2 = fig.add_subplot(gs[0, 1])
    class_names = ['Benign', 'Malignant']
    sns.barplot(
        x=class_names,
        y=predictions['class_probabilities'],
        ax=ax2
    )
    ax2.set_title('Prediction Confidence')
    ax2.set_ylim(0, 1)
    
    # Tissue graph
    ax3 = fig.add_subplot(gs[1, :])
    ax3.set_title('Tissue Graph Analysis')
    # Convert plotly figure to matplotlib
    graph_img = Image.open(io.BytesIO(graph_fig.to_image(format="png")))
    ax3.imshow(graph_img)
    ax3.axis('off')
    
    # Add metadata
    plt.figtext(
        0.02, 0.02,
        f"Analysis Date: {predictions['metadata']['date']}\n" +
        f"Model Version: {predictions['metadata']['model_version']}",
        fontsize=8
    )
    
    # Save to bytes buffer
    buf = io.BytesIO()
    plt.savefig(buf, format='pdf', bbox_inches='tight')
    buf.seek(0)
    
    return buf.getvalue()

def plot_training_metrics(
    metrics: dict,
    save_path: Optional[str] = None
) -> go.Figure:
    """Plot training metrics over time"""
    fig = go.Figure()
    
    # Add traces for each metric
    for metric_name, values in metrics.items():
        fig.add_trace(go.Scatter(
            y=values,
            name=metric_name,
            mode='lines'
        ))
    
    fig.update_layout(
        title='Training Metrics',
        xaxis_title='Epoch',
        yaxis_title='Value',
        hovermode='x'
    )
    
    if save_path:
        fig.write_html(save_path)
    
    return fig

================
File: tests/test_api.py
================
import pytest
from fastapi.testclient import TestClient
from src.api.endpoints import app
import io
from PIL import Image
import numpy as np
import json

@pytest.fixture
def client():
    """Create test client"""
    return TestClient(app)

@pytest.fixture
def sample_image():
    """Create sample image for testing"""
    img = Image.fromarray(
        np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    )
    img_byte_arr = io.BytesIO()
    img.save(img_byte_arr, format='PNG')
    img_byte_arr.seek(0)
    return img_byte_arr

class TestAPI:
    def test_health_check(self, client):
        response = client.get("/health")
        assert response.status_code == 200
        assert response.json()["status"] == "healthy"
        
    def test_model_info(self, client):
        response = client.get("/model-info")
        assert response.status_code == 200
        assert "model_type" in response.json()
        
    def test_predict_endpoint(self, client, sample_image):
        files = {"file": ("test.png", sample_image, "image/png")}
        response = client.post("/predict", files=files)
        
        assert response.status_code == 200
        data = response.json()
        assert "predictions" in data
        assert "visualizations" in data
        
        # Check prediction structure
        pred = data["predictions"]
        assert "class_probabilities" in pred
        assert "predicted_class" in pred
        assert "confidence" in pred
        
        # Validate probability values
        probs = pred["class_probabilities"]
        assert len(probs) == 2  # Binary classification
        assert all(0 <= p <= 1 for p in probs)
        assert abs(sum(probs) - 1.0) < 1e-6  # Sum to 1
        
    def test_batch_predict(self, client, sample_image):
        files = [
            ("files", ("test1.png", sample_image, "image/png")),
            ("files", ("test2.png", sample_image, "image/png"))
        ]
        response = client.post("/batch-predict", files=files)
        
        assert response.status_code == 200
        data = response.json()
        assert "results" in data
        assert len(data["results"]) == 2
        
    @pytest.mark.parametrize("file_ext", ["jpg", "tiff", "bmp"])
    def test_invalid_file_format(self, client, file_ext):
        invalid_image = io.BytesIO(b"invalid image content")
        files = {"file": (f"test.{file_ext}", invalid_image, "image/jpeg")}
        response = client.post("/predict", files=files)
        
        assert response.status_code == 400
        assert "Invalid file format" in response.json()["detail"]
        
    def test_error_handling(self, client):
        # Test with invalid file
        files = {"file": ("test.png", io.BytesIO(b""), "image/png")}
        response = client.post("/predict", files=files)
        assert response.status_code == 500
        
    def test_concurrent_requests(self, client, sample_image):
        import concurrent.futures
        
        def make_request():
            files = {"file": ("test.png", sample_image, "image/png")}
            return client.post("/predict", files=files)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(make_request) for _ in range(5)]
            responses = [f.result() for f in futures]
            
        assert all(r.status_code == 200 for r in responses)

================
File: tests/test_data.py
================
import pytest
import torch
from pathlib import Path
import numpy as np
from PIL import Image
from src.data.dataset import HistopathologyDataset, HistopathologyDataModule
from src.data.preprocessing import extract_patches, create_tissue_graph

@pytest.fixture
def sample_image():
    """Create sample image for testing"""
    return Image.fromarray(
        np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    )

@pytest.fixture
def sample_dataset(tmp_path):
    """Create temporary dataset for testing"""
    dataset_path = tmp_path / "dataset"
    for label in [0, 1]:
        path = dataset_path / str(label)
        path.mkdir(parents=True)
        for i in range(5):
            img = np.random.randint(0, 255, (50, 50, 3), dtype=np.uint8)
            img_path = path / f"image_{i}.png"
            Image.fromarray(img).save(img_path)
    return dataset_path

class TestPreprocessing:
    def test_extract_patches(self, sample_image):
        patches = extract_patches(
            sample_image,
            patch_size=50,
            num_patches=10
        )
        
        assert len(patches) == 10
        assert isinstance(patches[0], Image.Image)
        assert patches[0].size == (50, 50)
        
    def test_create_tissue_graph(self):
        patches = torch.randn(10, 3, 50, 50)
        graph = create_tissue_graph(patches)
        
        assert hasattr(graph, 'x')
        assert hasattr(graph, 'edge_index')
        assert hasattr(graph, 'pos')
        
class TestDataset:
    def test_initialization(self, sample_dataset):
        dataset = HistopathologyDataset(str(sample_dataset))
        assert len(dataset) == 10  # 5 images per class
        
    def test_getitem(self, sample_dataset):
        dataset = HistopathologyDataset(str(sample_dataset))
        item = dataset[0]
        
        assert 'patches' in item
        assert 'graph' in item
        assert 'label' in item
        assert isinstance(item['patches'], torch.Tensor)
        
    @pytest.mark.parametrize(
        "patch_size,num_patches",
        [(32, 50), (64, 25)]
    )
    def test_different_configurations(
        self,
        sample_dataset,
        patch_size,
        num_patches
    ):
        dataset = HistopathologyDataset(
            str(sample_dataset),
            patch_size=patch_size,
            num_patches=num_patches
        )
        item = dataset[0]
        
        assert item['patches'].shape == (
            num_patches,
            3,
            patch_size,
            patch_size
        )

class TestDataModule:
    def test_initialization(self, sample_dataset):
        datamodule = HistopathologyDataModule(str(sample_dataset))
        assert isinstance(datamodule, HistopathologyDataModule)
        
    def test_setup(self, sample_dataset):
        datamodule = HistopathologyDataModule(str(sample_dataset))
        datamodule.setup()
        
        assert hasattr(datamodule, 'train_dataset')
        assert hasattr(datamodule, 'val_dataset')
        assert hasattr(datamodule, 'test_dataset')
        
    def test_dataloaders(self, sample_dataset):
        datamodule = HistopathologyDataModule(str(sample_dataset))
        datamodule.setup()
        
        train_loader = datamodule.train_dataloader()
        val_loader = datamodule.val_dataloader()
        test_loader = datamodule.test_dataloader()
        
        assert isinstance(train_loader, torch.utils.data.DataLoader)
        assert isinstance(val_loader, torch.utils.data.DataLoader)
        assert isinstance(test_loader, torch.utils.data.DataLoader)

================
File: tests/test_models.py
================
import pytest
import torch
from src.models.mil import MILModel
from src.models.gnn import GNNModel
from src.models.combined_model import CombinedModel

@pytest.fixture
def sample_batch():
    """Create sample batch for testing"""
    batch_size = 2
    num_patches = 4
    patch_size = 50
    
    return {
        'patches': torch.randn(batch_size, num_patches, 3, patch_size, patch_size),
        'graph': {
            'x': torch.randn(batch_size * 10, 512),
            'edge_index': torch.randint(0, batch_size * 10, (2, 30)),
            'batch': torch.repeat_interleave(
                torch.arange(batch_size),
                repeats=10
            )
        },
        'label': torch.randint(0, 2, (batch_size,))
    }

class TestMILModel:
    def test_initialization(self):
        model = MILModel(num_classes=2)
        assert isinstance(model, MILModel)
        
    def test_forward_pass(self, sample_batch):
        model = MILModel(num_classes=2)
        logits, attention = model(sample_batch['patches'])
        
        assert logits.shape == (2, 2)  # (batch_size, num_classes)
        assert attention.shape == (2, 4, 1)  # (batch_size, num_patches, 1)
        
    def test_training_step(self, sample_batch):
        model = MILModel(num_classes=2)
        loss = model.training_step(sample_batch, 0)
        
        assert isinstance(loss, torch.Tensor)
        assert loss.requires_grad

class TestGNNModel:
    def test_initialization(self):
        model = GNNModel()
        assert isinstance(model, GNNModel)
        
    def test_forward_pass(self, sample_batch):
        model = GNNModel()
        logits, features = model(sample_batch['graph'])
        
        assert logits.shape == (2, 2)  # (batch_size, num_classes)
        assert features.shape == (2, 256)  # (batch_size, hidden_dim)
        
    def test_training_step(self, sample_batch):
        model = GNNModel()
        loss = model.training_step(sample_batch, 0)
        
        assert isinstance(loss, torch.Tensor)
        assert loss.requires_grad

class TestCombinedModel:
    def test_initialization(self):
        model = CombinedModel()
        assert isinstance(model, CombinedModel)
        
    def test_forward_pass(self, sample_batch):
        model = CombinedModel()
        logits, outputs = model(sample_batch)
        
        assert logits.shape == (2, 2)  # (batch_size, num_classes)
        assert 'mil_attention' in outputs
        assert 'gnn_features' in outputs
        
    def test_training_step(self, sample_batch):
        model = CombinedModel()
        loss = model.training_step(sample_batch, 0)
        
        assert isinstance(loss, torch.Tensor)
        assert loss.requires_grad
        
    @pytest.mark.parametrize("batch_size", [1, 4, 8])
    def test_different_batch_sizes(self, batch_size):
        model = CombinedModel()
        batch = {
            'patches': torch.randn(batch_size, 4, 3, 50, 50),
            'graph': {
                'x': torch.randn(batch_size * 10, 512),
                'edge_index': torch.randint(0, batch_size * 10, (2, 30)),
                'batch': torch.repeat_interleave(
                    torch.arange(batch_size),
                    repeats=10
                )
            },
            'label': torch.randint(0, 2, (batch_size,))
        }
        
        logits, outputs = model(batch)
        assert logits.shape == (batch_size, 2)
